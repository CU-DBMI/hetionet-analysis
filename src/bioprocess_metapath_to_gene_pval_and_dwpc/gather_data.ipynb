{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc183d5-7a0d-4a1a-b0b5-c87a5f54214b",
   "metadata": {},
   "source": [
    "# Biological Process and Gene Metapath Data Gathering\n",
    "\n",
    "- Each value from `BP.csv` is a source and each value from `Gene.csv` is a target. \n",
    "- Each source + target pairing may have a metapath which is found within `metapaths.csv`.\n",
    "- For each pair metapath we need the DWPC and p-value stored in a table for reference.\n",
    "- Ignore metapaths found within `metapaths_ignore.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb23bdc3-241a-47e0-8016-0533730762b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from itertools import product\n",
    "from typing import Generator, Iterator, Tuple\n",
    "\n",
    "import lancedb\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from pyarrow import csv\n",
    "\n",
    "from hetionet_utils.database import HetionetNeo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33492577-2f44-404f-b866-f76a54de28ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arrow_table_from_csv_column(file_path: str, column_name: str) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Loads a single-column Arrow Table from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str):\n",
    "            The path to the CSV file.\n",
    "        column_name (str):\n",
    "            The name of the column to extract.\n",
    "\n",
    "    Returns:\n",
    "        pa.Table:\n",
    "            Arrow Table with the specified column.\n",
    "    \"\"\"\n",
    "    table = csv.read_csv(file_path)\n",
    "    return table.select([column_name])\n",
    "\n",
    "\n",
    "def generate_combinations(\n",
    "    table_bioprocesses: pa.Table, table_genes: pa.Table, table_metapaths: pa.Table\n",
    ") -> Generator[Tuple[str, str, str], None, None]:\n",
    "    \"\"\"\n",
    "    Generates all possible combinations of IDs from three Arrow tables.\n",
    "\n",
    "    Args:\n",
    "        table_bioprocesses (pa.Table):\n",
    "            Arrow Table containing bioprocess IDs in an 'id' column.\n",
    "        table_genes (pa.Table):\n",
    "            Arrow Table containing gene IDs in an 'id' column.\n",
    "        table_metapaths (pa.Table):\n",
    "            Arrow Table containing metapath values in a 'metapath' column.\n",
    "\n",
    "    Yields:\n",
    "        Tuple[str, str, str]:\n",
    "            A tuple with a bioprocess ID, a gene ID, and a metapath value.\n",
    "    \"\"\"\n",
    "    for combo in product(\n",
    "        table_bioprocesses[\"id\"].to_pylist(),\n",
    "        table_genes[\"id\"].to_pylist(),\n",
    "        table_metapaths[\"metapath\"].to_pylist(),\n",
    "    ):\n",
    "        yield combo\n",
    "\n",
    "\n",
    "def process_in_chunks(\n",
    "    generator: Iterator[Tuple[str, str, str]], chunk_size: int = 1000\n",
    ") -> Iterator[pa.Table]:\n",
    "    \"\"\"\n",
    "    Processes combinations from a generator in smaller chunks\n",
    "    as Arrow Tables.\n",
    "\n",
    "    Args:\n",
    "        generator (Iterator[Tuple[str, str, str]]):\n",
    "            A generator that yields tuples of combinations.\n",
    "        chunk_size (int, optional):\n",
    "            The number of rows per chunk. Defaults to 1000.\n",
    "\n",
    "    Yields:\n",
    "        pa.Table:\n",
    "            An Arrow Table containing a chunk of combinations\n",
    "            with columns ['source_id', 'target_id', 'metapath'].\n",
    "    \"\"\"\n",
    "    chunk = []\n",
    "    for i, combo in enumerate(generator):\n",
    "        # Convert each combination to a tuple of strings to avoid None values\n",
    "        combo_tuple = tuple(str(x) if x is not None else \"\" for x in combo)\n",
    "        chunk.append(combo_tuple)\n",
    "\n",
    "        if (i + 1) % chunk_size == 0:\n",
    "            # Create Arrow Table from the chunk\n",
    "            yield pa.table(\n",
    "                {\n",
    "                    \"source_id\": [row[0] for row in chunk],\n",
    "                    \"target_id\": [row[1] for row in chunk],\n",
    "                    \"metapath\": [row[2] for row in chunk],\n",
    "                }\n",
    "            )\n",
    "            chunk = []\n",
    "\n",
    "    # Yield any remaining combinations as an Arrow Table\n",
    "    if chunk:\n",
    "        yield pa.table(\n",
    "            {\n",
    "                \"source_id\": [row[0] for row in chunk],\n",
    "                \"target_id\": [row[1] for row in chunk],\n",
    "                \"metapath\": [row[2] for row in chunk],\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22142383-8ae8-4c3a-b5ac-17c9f2dfc409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metapath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BPpGdAdG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BPpGdAeG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BPpGdAuG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BPpGeAdG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BPpGeAeG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   metapath\n",
       "5  BPpGdAdG\n",
       "6  BPpGdAeG\n",
       "7  BPpGdAuG\n",
       "8  BPpGeAdG\n",
       "9  BPpGeAeG"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gather metapaths which are not in the metapaths_ignore.csv\n",
    "df_metapaths = pd.read_csv(\"data/sources/metapaths.csv\")\n",
    "df_metapaths_ignore = pd.read_csv(\"data/sources/metapaths_ignore.csv\")\n",
    "df_metapaths = df_metapaths[\n",
    "    ~df_metapaths[\"metapath\"].isin(df_metapaths_ignore[\"metapath\"])\n",
    "]\n",
    "df_metapaths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b75acd4-2bb3-4adf-a851-d758e75b22b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of queries:  11203627115\n"
     ]
    }
   ],
   "source": [
    "# Load input CSV files into Arrow Tables\n",
    "table_bioprocesses = load_arrow_table_from_csv_column(\"data/sources/BP.csv\", \"id\")\n",
    "table_genes = load_arrow_table_from_csv_column(\"data/sources/Gene.csv\", \"id\")\n",
    "table_metapaths = pa.Table.from_pandas(df_metapaths)\n",
    "\n",
    "print(\n",
    "    \"Expected number of queries: \",\n",
    "    table_bioprocesses.num_rows * table_genes.num_rows * table_metapaths.num_rows,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "258960f1-1ea7-4365-b948-af69c6425d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "hetiocli = HetionetNeo4j()\n",
    "sample_result = hetiocli.get_metapath_data(\n",
    "    source_id=str(table_bioprocesses[0][0]),\n",
    "    target_id=int(str(table_genes[0][0])),\n",
    "    metapath=str(table_metapaths[0][0]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61dc2f7e-c6d0-4f4a-87e7-f9a5d8309f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-06T12:56:02Z WARN  lance::dataset] No existing dataset at /Users/dabu5788/Documents/work/hetionet-analysis/src/bioprocess_metapath_to_gene_pval_and_dwpc/data/results/bioprocess_and_gene_metapaths/bioprocess_gene_metapath_scores.lance, it will be created\n"
     ]
    }
   ],
   "source": [
    "# create results folder\n",
    "pathlib.Path(\"data/results\").mkdir(exist_ok=True)\n",
    "\n",
    "# Initialize your LanceDB database and table\n",
    "db = lancedb.connect(\"data/results/bioprocess_and_gene_metapaths\")\n",
    "table_name = \"bioprocess_gene_metapath_scores\"\n",
    "\n",
    "# create table, overwriting previous results\n",
    "db.create_table(\n",
    "    table_name,\n",
    "    schema=pa.Table.from_pandas(sample_result).schema,\n",
    "    mode=\"overwrite\",\n",
    ")\n",
    "\n",
    "table = db.open_table(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea0c56c1-ef9c-487b-80ae-296a55b2682a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding chunk 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk_table \u001b[38;5;129;01min\u001b[39;00m process_in_chunks(generator, chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000000\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# add the chunk to the table\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdding chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/work/hetionet-analysis/.venv/lib/python3.12/site-packages/lancedb/table.py:1519\u001b[0m, in \u001b[0;36mLanceTable.add\u001b[0;34m(self, data, mode, on_bad_vectors, fill_value)\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# Access the dataset_mut property to ensure that the dataset is mutable.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ref\u001b[38;5;241m.\u001b[39mdataset_mut\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ref\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m lance\u001b[38;5;241m.\u001b[39mwrite_dataset(\n\u001b[0;32m-> 1519\u001b[0m     data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_uri, schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m, mode\u001b[38;5;241m=\u001b[39mmode\n\u001b[1;32m   1520\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/work/hetionet-analysis/.venv/lib/python3.12/site-packages/lancedb/table.py:1117\u001b[0m, in \u001b[0;36mLanceTable.schema\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mschema\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pa\u001b[38;5;241m.\u001b[39mSchema:\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the schema of the table.\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \n\u001b[1;32m   1113\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;124;03m    pa.Schema\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;124;03m        A PyArrow schema object.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mschema\n",
      "File \u001b[0;32m~/Documents/work/hetionet-analysis/.venv/lib/python3.12/site-packages/lancedb/table.py:1099\u001b[0m, in \u001b[0;36mLanceTable._dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LanceDataset:\n\u001b[0;32m-> 1099\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\n",
      "File \u001b[0;32m~/Documents/work/hetionet-analysis/.venv/lib/python3.12/site-packages/lancedb/table.py:984\u001b[0m, in \u001b[0;36m_LanceLatestDatasetRef.dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdataset\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LanceDataset:\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset:\n\u001b[0;32m--> 984\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mlance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_cache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_cache_size\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_consistency_check \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_consistency_interval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/work/hetionet-analysis/.venv/lib/python3.12/site-packages/lance/__init__.py:92\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(uri, version, asof, block_size, commit_lock, index_cache_size, storage_options)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdataset\u001b[39m(\n\u001b[1;32m     51\u001b[0m     uri: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     52\u001b[0m     version: Optional[\u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     storage_options: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     58\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LanceDataset:\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    Opens the Lance dataset from the address specified.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m        used to store connection parameters like credentials, endpoint, etc.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mLanceDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommit_lock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_lock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_cache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_cache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m asof \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m         ts_cutoff \u001b[38;5;241m=\u001b[39m sanitize_ts(asof)\n",
      "File \u001b[0;32m~/Documents/work/hetionet-analysis/.venv/lib/python3.12/site-packages/lance/dataset.py:166\u001b[0m, in \u001b[0;36mLanceDataset.__init__\u001b[0;34m(self, uri, version, block_size, index_cache_size, metadata_cache_size, commit_lock, storage_options, serialized_manifest)\u001b[0m\n\u001b[1;32m    164\u001b[0m uri \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(uri) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(uri, Path) \u001b[38;5;28;01melse\u001b[39;00m uri\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_uri \u001b[38;5;241m=\u001b[39m uri\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ds \u001b[38;5;241m=\u001b[39m \u001b[43m_Dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_cache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_cache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_lock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserialized_manifest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate combinations\n",
    "generator = generate_combinations(table_bioprocesses, table_genes, table_metapaths)\n",
    "\n",
    "count = 1\n",
    "# Process and print chunks\n",
    "for chunk_table in process_in_chunks(generator, chunk_size=5000000):\n",
    "    # add the chunk to the table\n",
    "    print(f\"Adding chunk {count}\")\n",
    "    table.add(chunk_table)\n",
    "    count += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828431ff-398c-4c23-9fa7-f65a3925597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After inserting all chunks, show the shape of the table\n",
    "num_rows = table.count()\n",
    "num_columns = len(table.schema().names)\n",
    "\n",
    "print(f\"Table shape: ({num_rows}, {num_columns})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
